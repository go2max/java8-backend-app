Методы оптимизации в негладких задачах

4 questions
1
point
1.

Какие из перечисленных подходов к задаче поиска минимума не требуют существования градиента у оптимизируемой функции?

Метод градиентного спуска

Дифференциальная эволюция

Метод имитации отжига

Аналитическое нахождение точки минимума с помощью необходимого условия экстремума
1
point
2.

Как вы думаете, зачем нужна стадия мутации в генетических алгоритмах?

Чтобы с большей точностью двигаться на каждом шаге по градиенту сглаженной функции

Чтобы лучше попадать в локальные минимумы

Чтобы популяция не вырождалась слишком быстро в набор очень похожих друг на друга векторов и был некоторый шанс выбивания из локальных минимумов
1
point
3.

Почему при поиске минимума методом имитации отжига допускаются переходы в точки, в которых функция принимает большие значения, нежели в текущей?

Чтобы метод быстрее сходился

Чтобы метод мог иногда выбираться из локальных минимумов

Чтобы более точно моделировать физический процесс отжига
1
point
4.

В каких случаях стоит применять методы оптимизации, не использующие градиент?

В случаях, когда у функции нет градиента, т.к. в случае его наличия скорее всего неградиентному методу потребуется больше итераций, чем градиентному

Во всех случаях, т.к. очень часто неградиентные методы сходятся быстрее градиентных

В случаях, когда у функции нет градиента, т.к. в случае его наличия скорее всего неградиентному методу потребуется больше итераций или вычислений значения функции, чем градиентному. А также в случаях, когда нужно найти глобальный экстремум функции, а градиентные методы скорее всего будут попадать в локальный.
3 questions unanswered
