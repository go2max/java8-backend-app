# Реализуйте стохастический градиентный спуск, то есть методы SGD (stochastic gradient descent) и update_mini_batch класса Neuron. Когда вы решите сдать задачу, вам нужно будет просто скопировать соответствующие функции (которые вы написали в ноутбуке ) сюда. Копируем без учёта отступов; шаблон в поле ввода ответа уже будет, ориентируйтесь по нему. Сигнатура функции указана в ноутбуке, она остаётся неизменной.

# Задание получилось очень сложным, особенно для тех, у кого мало опыта программирования. Внимательно читайте комментарии в предоставленном коде, чтобы понять, что требуется от ваших функций. Главное - не спешите при написании кода, это приводит к обидным ошибкам и огромным временным затратам.

# SGD реализует основной цикл алгоритма. Должен возвращать 1, если градиентный спуск сошёлся, и 0 — если максимальное число итераций было достигнуто раньше, чем изменения в целевой функции стали достаточно малы.

# update_mini_batch считает градиент и обновляет веса нейрона на основе всей переданной ему порции данных, кроме того, возвращает 1, если алгоритм сошелся (абсолютное значение изменения целевой функции до и после обновления весов <

# eps), иначе возвращает 0.

# Необходимые внешние методы (compute_grad_analytically, J_quadratic) уже определены чуть ниже класса Neuron.

# Вам могут быть полезны такие функции, как:

# np.arange - создать последовательность (хотя можно обойтись и просто list(range( ... )))

# np.random.shuffle - перемешать последовательность

# np.random.choice - случайным образом выбрать нужное количество элементов из последовательности

# Если чувствуете, что решение получается громоздким (функция SGD занимает сильно больше 10 строчек) - можно повторить урок по numpy. По крайней мере, не забывайте, что если X это матрица (np.array со shape = (n, m)), а idx = [1, 5, 3], то X[idx] вернёт вам новую матрицу с тремя соответствующими строчками из X. Кроме того, X[3:5] вернёт вам строки c индексами 3 и 4 (не забывайте, что у нас есть еще нулевая строка). Обратите внимание, что если вы при такой индексации выйдете за границы массива - ошибки не будет, вернётся пустой или неполный (по сравнению с тем, что вы ожидали) набор строк.

# Наиболее частые ошибки:

# Неправильное формирование батча. Батч должен формироваться заново перед каждым вызовом update_mini_batch.

# Неправильная проверка условия выхода из цикла (превышения количества допустимых вызовов update_mini_batch )

# Неправильная проверка условия схождения алгоритма в update_mini_batch

# Самостоятельное переписывание (вместо переиспользования) предоставленных функций/методов

# Отсутствие self. перед обращением к атрибутам / методам класса

# Ошибки по невнимательности (впечатляющее разнообразие, в том числе: выходы за границы массива, формирование батча только по X, независимое перемешивание X и y, путаница с размерностями и индексацией, и многое другое ... )

# P.S.

# Если очень долго не найти ошибку - напишите псевдокод на бумажке, перепишите функцию "с нуля". Может оказаться быстрее чем выискивать какую-нибудь коварную мелочь. Читайте комментарии, там очень много полезного. Иногда, в конце концов, полезно отвлечься, ключевая идея может совершенно неожиданно прийти "за чашечкой чая". Главное - не отчаивайтесь, удачи!

import numpy as np

def SGD(self, X, y, batch_size, learning_rate=0.1, eps=1e-6, max_steps=200):
    for e in range(max_steps):
        epoch = list(range(len(X)))
        np.random.shuffle(epoch)
        for batch in [epoch[i:i + batch_size] for i in range(0, len(epoch), batch_size)]:
            if self.update_mini_batch(X[batch], y[batch], learning_rate, eps) != 0:
                return 1
    return 0

def update_mini_batch(self, X, y, learning_rate, eps):
    before = J_quadratic(self, X, y)
    grad = compute_grad_analytically(self, X, y)
    dw = -grad * learning_rate
    self.w += dw
    after = J_quadratic(self, X, y)
    return 1 if abs(after - before) < eps else 0